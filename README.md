# ğŸ® RL Galaxian - Deep Reinforcement Learning

Proyecto completo de **Reinforcement Learning** para entrenar agentes que jueguen Galaxian usando **Double DQN**, **Actor-Critic (A2C)** y **PPO**.

## ğŸš€ CaracterÃ­sticas

- âœ… **Double DQN**: Reduce sobreestimaciÃ³n de Q-values usando dos redes (policy y target)
- âœ… **Actor-Critic (A2C)**: Policy gradient con funciÃ³n de valor
- âœ… **PPO (Proximal Policy Optimization)**: Algoritmo state-of-the-art con clipping de ratio
- âœ… **OptimizaciÃ³n por reward**: Guarda mejor modelo basado en promedio mÃ³vil de recompensas
- âœ… **Memory-optimized**: Replay buffer con almacenamiento uint8 (4x menos memoria)
- âœ… **Reentrenamiento**: Carga modelo + buffer para continuar entrenando
- âœ… **Jupyter Notebooks**: Interfaz interactiva para entrenamiento y anÃ¡lisis
- âœ… **Video recording**: Graba episodios del agente entrenado

## ğŸ“ Estructura del Proyecto

```
RL_Galaxian/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ dqn.py              # Red neuronal Double DQN
â”‚   â”œâ”€â”€ agent.py            # Agente DQN con train_step y save/load
â”‚   â”œâ”€â”€ replay.py           # Replay buffer optimizado (uint8)
â”‚   â”œâ”€â”€ actor_critic.py     # Red Actor-Critic compartida
â”‚   â”œâ”€â”€ ac_agent.py         # Agente A2C
â”‚   â”œâ”€â”€ train.py            # FunciÃ³n de entrenamiento DQN
â”‚   â”œâ”€â”€ train_ac.py         # FunciÃ³n de entrenamiento A2C
â”‚   â”œâ”€â”€ record.py           # GrabaciÃ³n de videos
â”‚   â””â”€â”€ utils.py            # Preprocesamiento y utilidades
â”œâ”€â”€ train_dqn.ipynb         # Notebook: Entrenamiento Double DQN
â”œâ”€â”€ train_ac_notebook.ipynb # Notebook: Entrenamiento Actor-Critic
â”œâ”€â”€ PPO_RL.ipynb            # Notebook: Entrenamiento PPO
â”œâ”€â”€ train.py                # CLI: Entrenar DQN
â”œâ”€â”€ train_ac_cli.py         # CLI: Entrenar Actor-Critic
â”œâ”€â”€ record_cli.py           # CLI: Grabar videos
â””â”€â”€ requirements.txt        # Dependencias
```

## ğŸ› ï¸ InstalaciÃ³n

```powershell
# Clonar repositorio
git clone <repo-url>
cd RL_Galaxian

# Crear entorno virtual (recomendado)
python -m venv venv
.\venv\Scripts\Activate.ps1

# Instalar dependencias
pip install -r requirements.txt
```

## ğŸ¯ Uso RÃ¡pido

### OpciÃ³n 1: Jupyter Notebooks (Recomendado)

```powershell
# Abrir notebooks
jupyter notebook

# Ejecutar:
# - train_dqn.ipynb: Para Double DQN
# - train_ac_notebook.ipynb: Para Actor-Critic
# - PPO_RL.ipynb: Para PPO
```

### OpciÃ³n 2: Scripts CLI

**Entrenar Double DQN:**
```powershell
python train.py --episodes 500 --email tu@email.com
```

**Entrenar Actor-Critic:**
```powershell
python train_ac_cli.py --episodes 500 --email tu@email.com
```

**Grabar video del agente:**
```powershell
python record_cli.py --model checkpoints_dqn\best_model_tu.pth --email tu@email.com
```

## ğŸ§  Algoritmos Implementados

### Double DQN
- **Arquitectura**: 4 capas convolucionales + 4 capas fully-connected
- **OptimizaciÃ³n**: Adam, lr=1e-4
- **ExploraciÃ³n**: Îµ-greedy con decay exponencial
- **Buffer**: 100K experiencias (uint8 para eficiencia)
- **Target update**: Cada 1000 steps
- **Early stopping**: Basado en moving average de rewards

### Actor-Critic (A2C)
- **Arquitectura**: Capas conv compartidas, heads separados (actor/critic)
- **OptimizaciÃ³n**: Adam, lr=3e-4
- **Entropy regularization**: 0.05 (fomenta exploraciÃ³n)
- **Advantages**: Normalizadas para estabilidad
- **Sin reward clipping**: Aprende valores reales de Galaxian
- **Early stopping**: Basado en moving average de rewards

### PPO (Proximal Policy Optimization)
- **Arquitectura**: Red compartida con heads actor/critic separados
- **OptimizaciÃ³n**: Adam, lr=3e-4
- **Clipped objective**: Ratio clipping (Îµ=0.2) para actualizaciones estables
- **Multiple epochs**: 4 Ã©pocas de actualizaciÃ³n por batch
- **GAE (Generalized Advantage Estimation)**: Î»=0.95 para reducir varianza
- **Entropy bonus**: 0.01 para exploraciÃ³n
- **Value function clipping**: Estabiliza aprendizaje del crÃ­tico
- **State-of-the-art**: Mejor balance exploraciÃ³n/explotaciÃ³n

## ğŸ“Š HiperparÃ¡metros Clave

| ParÃ¡metro | Double DQN | Actor-Critic | PPO |
|-----------|------------|--------------|-----|
| Learning Rate | 1e-4 | 3e-4 | 3e-4 |
| Batch Size | 32 | N/A (on-policy) | 256 |
| Gamma (Î³) | 0.99 | 0.99 | 0.99 |
| Epsilon start | 1.0 | N/A | N/A |
| Epsilon end | 0.10 | N/A | N/A |
| Clip range (Îµ) | N/A | N/A | 0.2 |
| Entropy coef | N/A | 0.05 | 0.01 |
| GAE Î» | N/A | N/A | 0.95 |
| Update epochs | N/A | 1 | 4 |
| MA Window | 20 | 20 | 20 |

## ğŸ“ˆ MÃ©tricas y VisualizaciÃ³n

Los notebooks generan automÃ¡ticamente:
- GrÃ¡ficas de rewards por episodio
- TD Loss / Actor-Critic losses / PPO losses
- Epsilon decay (DQN) / Entropy (A2C/PPO)
- Policy ratio y clipping (PPO)
- Moving average de rewards
- GrÃ¡ficas guardadas en `checkpoints_*/`

## ğŸ’¾ Checkpoints

Los modelos se guardan en:
- `checkpoints_dqn/`: Modelos Double DQN
- `checkpoints_ac/`: Modelos Actor-Critic
- `checkpoints_ppo/`: Modelos PPO

Tipos de checkpoints:
- `best_model_*.pth`: Mejor modelo (mayor MA de rewards)
- `final_model_*.pth`: Modelo al finalizar entrenamiento
- `checkpoint_*_ep{N}_*.pth`: Checkpoints periÃ³dicos

## ğŸ¬ Videos

Los videos se guardan en:
- `videos_dqn/`: Videos de agente DQN
- `videos_ac/`: Videos de agente Actor-Critic
- `videos_ppo/`: Videos de agente PPO

Formato: MP4 con metadata del episodio

## ğŸ”§ Preprocesamiento

1. ConversiÃ³n a escala de grises
2. Resize a 84x84
3. NormalizaciÃ³n [0, 1]
4. Frame stacking (4 frames)

## ğŸ“ Notas Importantes

- **Rewards no clipeados en A2C**: Aprende valores reales de Galaxian (+30, +60, +200)
- **DQN usa rewards clipeados**: [-1, +1] para estabilidad
- **Replay buffer en uint8**: Ahorra 75% de memoria
- **Early stopping automÃ¡tico**: Detiene si no mejora en 200 episodios
- **Reentrenamiento**: Soporta carga de modelo + buffer para continuar

## ğŸ› Troubleshooting

**Error de memoria:**
- Reduce `BUFFER_SIZE` o `EPISODES`
- El buffer ya estÃ¡ optimizado con uint8

**Modelo no aprende:**
- Verifica que `MA_WINDOW` sea apropiado
- Aumenta `EPISODES` para mÃ¡s exploraciÃ³n
- Revisa grÃ¡ficas de entropy/epsilon para asegurar exploraciÃ³n

**Error de checkpoint:**
- AsegÃºrate de usar PyTorch 2.6+
- Los checkpoints incluyen `weights_only=True` para seguridad

## ğŸ“ Autor

Proyecto de Reinforcement Learning - UVG
Email: ang23010@uvg.edu.gt

## ğŸ“œ Licencia

MIT License - Libre para uso acadÃ©mico y personal

Readme generado con IA
