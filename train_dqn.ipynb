{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc321d58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paquetes faltantes: ['opencv_python']\n",
      "Inst√°lalos con: pip install -r requirements.txt\n"
     ]
    }
   ],
   "source": [
    "# Verificar dependencias\n",
    "import importlib\n",
    "missing = []\n",
    "for pkg in ('gymnasium','ale_py','shimmy','torch','torchvision','opencv_python','matplotlib','numpy'):\n",
    "    try:\n",
    "        importlib.import_module(pkg)\n",
    "    except Exception:\n",
    "        missing.append(pkg)\n",
    "if missing:\n",
    "    print('Paquetes faltantes:', missing)\n",
    "    print('Inst√°lalos con: pip install -r requirements.txt')\n",
    "else:\n",
    "    print('‚úì Dependencias instaladas')\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "torch.serialization.add_safe_globals([np._core.multiarray.scalar])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1710c3d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: c:\\Users\\carlo\\Downloads\\RL_Galaxian\n"
     ]
    }
   ],
   "source": [
    "# Imports y configuraci√≥n\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from collections import deque\n",
    "import gymnasium as gym\n",
    "\n",
    "proj_root = Path.cwd()\n",
    "if str(proj_root) not in sys.path:\n",
    "    sys.path.insert(0, str(proj_root))\n",
    "\n",
    "from src.train import train_agent\n",
    "from src.record import record_episode\n",
    "from src.agent import DQNAgent\n",
    "from src.utils import preprocess_frame, stack_frames\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print('Project root:', proj_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66bfd8a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Par√°metros Double DQN:\n",
      " EPISODES= 500\n",
      " BATCH_SIZE= 32\n",
      " BUFFER_SIZE= 100000\n",
      " TARGET_UPDATE= 1000\n",
      " EARLY_STOP_PATIENCE= 200\n",
      " MA_WINDOW= 20\n",
      " DEVICE= cpu\n"
     ]
    }
   ],
   "source": [
    "# Par√°metros de entrenamiento Double DQN\n",
    "EPISODES = 500\n",
    "EMAIL = 'ang23010@uvg.edu.gt'\n",
    "CHECKPOINT_DIR = 'checkpoints_dqn'\n",
    "ENV_NAME = 'ALE/Galaxian-v5'\n",
    "DEVICE = 'cpu'\n",
    "EARLY_STOP_PATIENCE = 200\n",
    "MA_WINDOW = 20\n",
    "SAVE_EVERY = 100\n",
    "BATCH_SIZE = 32\n",
    "BUFFER_SIZE = 100000\n",
    "TARGET_UPDATE = 1000\n",
    "\n",
    "print('Par√°metros Double DQN:')\n",
    "print(f' EPISODES= {EPISODES}')\n",
    "print(f' BATCH_SIZE= {BATCH_SIZE}')\n",
    "print(f' BUFFER_SIZE= {BUFFER_SIZE}')\n",
    "print(f' TARGET_UPDATE= {TARGET_UPDATE}')\n",
    "print(f' EARLY_STOP_PATIENCE= {EARLY_STOP_PATIENCE}')\n",
    "print(f' MA_WINDOW= {MA_WINDOW}')\n",
    "print(f' DEVICE= {DEVICE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f886f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenamiento Double DQN\n",
    "agent, results = train_agent(\n",
    "    episodes=EPISODES,\n",
    "    email=EMAIL,\n",
    "    checkpoint_dir=CHECKPOINT_DIR,\n",
    "    env_name=ENV_NAME,\n",
    "    early_stop_patience=EARLY_STOP_PATIENCE,\n",
    "    ma_window=MA_WINDOW,\n",
    "    save_every=SAVE_EVERY,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    buffer_size=BUFFER_SIZE,\n",
    "    target_update=TARGET_UPDATE,\n",
    "    device=DEVICE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0abc4b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar m√©tricas de entrenamiento\n",
    "rewards = results.get('rewards', [])\n",
    "losses = results.get('losses', [])\n",
    "epsilon = results.get('epsilon', [])\n",
    "durations = results.get('durations', [])\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "\n",
    "# Rewards\n",
    "axes[0, 0].plot(rewards)\n",
    "axes[0, 0].set_title('Rewards por Episodio')\n",
    "axes[0, 0].set_xlabel('Episodio')\n",
    "axes[0, 0].set_ylabel('Reward Total')\n",
    "axes[0, 0].grid(True)\n",
    "\n",
    "# Loss\n",
    "axes[0, 1].plot(losses)\n",
    "axes[0, 1].set_title('TD Loss')\n",
    "axes[0, 1].set_xlabel('Episodio')\n",
    "axes[0, 1].set_ylabel('Loss')\n",
    "axes[0, 1].grid(True)\n",
    "\n",
    "# Epsilon (exploraci√≥n)\n",
    "axes[1, 0].plot(epsilon)\n",
    "axes[1, 0].set_title('Epsilon (Exploraci√≥n)')\n",
    "axes[1, 0].set_xlabel('Episodio')\n",
    "axes[1, 0].set_ylabel('Epsilon')\n",
    "axes[1, 0].grid(True)\n",
    "\n",
    "# Moving average de rewards\n",
    "if len(rewards) >= MA_WINDOW:\n",
    "    ma_rewards = [np.mean(rewards[max(0, i-MA_WINDOW+1):i+1]) for i in range(len(rewards))]\n",
    "    axes[1, 1].plot(ma_rewards, color='green')\n",
    "    axes[1, 1].set_title(f'MA Rewards (ventana={MA_WINDOW})')\n",
    "    axes[1, 1].set_xlabel('Episodio')\n",
    "    axes[1, 1].set_ylabel('MA Reward')\n",
    "    axes[1, 1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(CHECKPOINT_DIR, 'training_metrics_dqn.png'), dpi=300)\n",
    "plt.show()\n",
    "\n",
    "print(f'\\nüìä Mejor reward promedio: {max(ma_rewards) if len(rewards) >= MA_WINDOW else max(rewards):.1f}')\n",
    "print(f'üìà Reward final (√∫ltimos {MA_WINDOW} eps): {np.mean(rewards[-MA_WINDOW:]):.1f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498124c2",
   "metadata": {},
   "source": [
    "## Reentrenamiento (Continuar desde modelo guardado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f4633b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Par√°metros de reentrenamiento\n",
    "RESUME_MODEL = os.path.join(CHECKPOINT_DIR, f'final_model_dqn_ang23010.pth')\n",
    "ADDITIONAL_EPISODES = 200\n",
    "EARLY_STOP_PATIENCE_RESUME = 100\n",
    "MA_WINDOW_RESUME = 20\n",
    "SAVE_EVERY_RESUME = 50\n",
    "BATCH_SIZE_RESUME = 32\n",
    "TARGET_UPDATE_RESUME = 1000\n",
    "EPSILON_START_RESUME = 0.1  # Empezar con menos exploraci√≥n\n",
    "\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "# Crear entorno y agente\n",
    "env = gym.make(ENV_NAME)\n",
    "n_actions = env.action_space.n\n",
    "state_shape = (4, 84, 84)\n",
    "\n",
    "agent_resume = DQNAgent(\n",
    "    state_shape, \n",
    "    n_actions, \n",
    "    buffer_size=BUFFER_SIZE,\n",
    "    batch_size=BATCH_SIZE_RESUME,\n",
    "    device=DEVICE\n",
    ")\n",
    "\n",
    "# Cargar modelo Y buffer\n",
    "if os.path.exists(RESUME_MODEL):\n",
    "    print(f'üìÇ Cargando modelo DQN desde {RESUME_MODEL}')\n",
    "    agent_resume.load(RESUME_MODEL, load_buffer=True)\n",
    "    print(f'‚úì Modelo cargado exitosamente')\n",
    "    print(f'‚úì Replay buffer cargado con {len(agent_resume.memory)} experiencias')\n",
    "else:\n",
    "    print(f'‚ùå Modelo no encontrado: {RESUME_MODEL}')\n",
    "    raise FileNotFoundError(f'No existe {RESUME_MODEL}')\n",
    "\n",
    "# Tracking\n",
    "best_ma = -np.inf\n",
    "no_improve = 0\n",
    "stacked = deque(maxlen=4)\n",
    "\n",
    "rewards = []\n",
    "losses = []\n",
    "epsilon_values = []\n",
    "durations = []\n",
    "\n",
    "# Configurar epsilon inicial\n",
    "agent_resume.epsilon = EPSILON_START_RESUME\n",
    "steps_done = 0\n",
    "\n",
    "# Loop de reentrenamiento\n",
    "for ep in range(1, ADDITIONAL_EPISODES + 1):\n",
    "    state, _ = env.reset()\n",
    "    processed = preprocess_frame(state)\n",
    "    state_stack = stack_frames(stacked, processed, True)\n",
    "\n",
    "    done = False\n",
    "    truncated = False\n",
    "    ep_reward = 0.0\n",
    "    ep_loss = 0.0\n",
    "    ep_steps = 0\n",
    "    loss_count = 0\n",
    "\n",
    "    while not (done or truncated):\n",
    "        action = agent_resume.select_action(state_stack)\n",
    "        next_state, reward, done, truncated, _ = env.step(action)\n",
    "\n",
    "        processed = preprocess_frame(next_state)\n",
    "        next_stack = stack_frames(stacked, processed, False)\n",
    "\n",
    "        agent_resume.memory.push(state_stack, action, reward, next_stack, done or truncated)\n",
    "\n",
    "        # Train\n",
    "        if len(agent_resume.memory) >= BATCH_SIZE_RESUME:\n",
    "            loss = agent_resume.train_step()\n",
    "            if loss is not None:\n",
    "                ep_loss += loss\n",
    "                loss_count += 1\n",
    "\n",
    "        ep_reward += reward\n",
    "        ep_steps += 1\n",
    "        steps_done += 1\n",
    "        state_stack = next_stack\n",
    "\n",
    "        # Update target network\n",
    "        if steps_done % TARGET_UPDATE_RESUME == 0:\n",
    "            agent_resume.update_target_network()\n",
    "\n",
    "    # Decay epsilon\n",
    "    agent_resume.epsilon = max(0.01, agent_resume.epsilon * 0.995)\n",
    "\n",
    "    # Promediar loss\n",
    "    avg_loss = ep_loss / loss_count if loss_count > 0 else 0.0\n",
    "\n",
    "    rewards.append(ep_reward)\n",
    "    losses.append(avg_loss)\n",
    "    epsilon_values.append(agent_resume.epsilon)\n",
    "    durations.append(ep_steps)\n",
    "\n",
    "    # Moving average\n",
    "    if len(rewards) >= MA_WINDOW_RESUME:\n",
    "        ma = np.mean(rewards[-MA_WINDOW_RESUME:])\n",
    "    else:\n",
    "        ma = np.mean(rewards) if rewards else -np.inf\n",
    "\n",
    "    print(f'Resume Ep {ep} | Reward: {ep_reward:.1f} | MA: {ma:.1f} | '\n",
    "          f'Loss: {avg_loss:.4f} | Œµ: {agent_resume.epsilon:.3f} | Steps: {ep_steps}')\n",
    "\n",
    "    # Guardar mejor modelo\n",
    "    if ma > best_ma:\n",
    "        best_ma = ma\n",
    "        no_improve = 0\n",
    "        email_prefix = EMAIL.split('@')[0]\n",
    "        best_path = os.path.join(CHECKPOINT_DIR, f'best_model_dqn_{email_prefix}.pth')\n",
    "        agent_resume.save(best_path, save_buffer=True)\n",
    "        print(f'  ‚úì Nuevo mejor MA: {ma:.1f} ‚Üí Modelo guardado')\n",
    "    else:\n",
    "        no_improve += 1\n",
    "\n",
    "    # Checkpoints peri√≥dicos\n",
    "    if ep % SAVE_EVERY_RESUME == 0:\n",
    "        email_prefix = EMAIL.split('@')[0]\n",
    "        cp_path = os.path.join(CHECKPOINT_DIR, f'checkpoint_dqn_resume_ep{ep}_{email_prefix}.pth')\n",
    "        agent_resume.save(cp_path, save_buffer=True)\n",
    "        print(f'  ‚Üí Checkpoint guardado: ep{ep}')\n",
    "\n",
    "    # Early stopping\n",
    "    if no_improve >= EARLY_STOP_PATIENCE_RESUME:\n",
    "        print(f'Early stopping: {no_improve} episodios sin mejora')\n",
    "        break\n",
    "\n",
    "# Guardar modelo final\n",
    "email_prefix = EMAIL.split('@')[0]\n",
    "final_path = os.path.join(CHECKPOINT_DIR, f'final_model_dqn_{email_prefix}.pth')\n",
    "agent_resume.save(final_path, save_buffer=True)\n",
    "print(f'\\n‚úì Reentrenamiento completado. Modelo final: {final_path}')\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af8bb60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar m√©tricas de reentrenamiento\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "\n",
    "# Rewards\n",
    "axes[0, 0].plot(rewards)\n",
    "axes[0, 0].set_title('Rewards (Reentrenamiento)')\n",
    "axes[0, 0].set_xlabel('Episodio')\n",
    "axes[0, 0].set_ylabel('Reward Total')\n",
    "axes[0, 0].grid(True)\n",
    "\n",
    "# Loss\n",
    "axes[0, 1].plot(losses)\n",
    "axes[0, 1].set_title('TD Loss (Reentrenamiento)')\n",
    "axes[0, 1].set_xlabel('Episodio')\n",
    "axes[0, 1].set_ylabel('Loss')\n",
    "axes[0, 1].grid(True)\n",
    "\n",
    "# Epsilon\n",
    "axes[1, 0].plot(epsilon_values)\n",
    "axes[1, 0].set_title('Epsilon (Reentrenamiento)')\n",
    "axes[1, 0].set_xlabel('Episodio')\n",
    "axes[1, 0].set_ylabel('Epsilon')\n",
    "axes[1, 0].grid(True)\n",
    "\n",
    "# MA Rewards\n",
    "if len(rewards) >= MA_WINDOW_RESUME:\n",
    "    ma_rewards = [np.mean(rewards[max(0, i-MA_WINDOW_RESUME+1):i+1]) for i in range(len(rewards))]\n",
    "    axes[1, 1].plot(ma_rewards, color='green')\n",
    "    axes[1, 1].set_title(f'MA Rewards (ventana={MA_WINDOW_RESUME})')\n",
    "    axes[1, 1].set_xlabel('Episodio')\n",
    "    axes[1, 1].set_ylabel('MA Reward')\n",
    "    axes[1, 1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(CHECKPOINT_DIR, 'retrain_metrics_dqn.png'), dpi=300)\n",
    "plt.show()\n",
    "\n",
    "print(f'\\nüìä Mejor reward promedio: {max(ma_rewards) if len(rewards) >= MA_WINDOW_RESUME else max(rewards):.1f}')\n",
    "print(f'üìà Reward final (√∫ltimos {MA_WINDOW_RESUME} eps): {np.mean(rewards[-MA_WINDOW_RESUME:]):.1f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28938199",
   "metadata": {},
   "source": [
    "## Grabar episodio con el mejor modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "27f2a873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üé¨ Cargando mejor modelo DQN desde: C:\\Users\\carlo\\Downloads\\RL_Galaxian\\checkpoints\\best_model_ang23010.pth\n",
      "Video saved: videos_dqn\\ang23010_20251120232612_300.mp4\n",
      "Score: 300 | Frames: 379\n",
      "\n",
      "‚úì Video guardado en: videos_dqn\\ang23010_20251120232612_300.mp4\n",
      "Video saved: videos_dqn\\ang23010_20251120232612_300.mp4\n",
      "Score: 300 | Frames: 379\n",
      "\n",
      "‚úì Video guardado en: videos_dqn\\ang23010_20251120232612_300.mp4\n"
     ]
    }
   ],
   "source": [
    "# Grabar episodio con mejor modelo DQN\n",
    "email_prefix = EMAIL.split('@')[0]\n",
    "best_model_path = r\"C:\\Users\\carlo\\Downloads\\RL_Galaxian\\checkpoints\\best_model_ang23010.pth\"\n",
    "\n",
    "if os.path.exists(best_model_path):\n",
    "    print(f'üé¨ Cargando mejor modelo DQN desde: {best_model_path}')\n",
    "    \n",
    "    state_shape = (4, 84, 84)\n",
    "    n_actions = 6\n",
    "    \n",
    "    agent_test = DQNAgent(state_shape, n_actions, device=DEVICE)\n",
    "    agent_test.load(best_model_path, load_buffer=False)\n",
    "    \n",
    "    video_path = record_episode(agent_test, email=EMAIL, output_dir='videos_dqn', env_name=ENV_NAME)\n",
    "    print(f'\\n‚úì Video guardado en: {video_path}')\n",
    "else:\n",
    "    print(f'‚ùå No se encontr√≥ el modelo: {best_model_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596eca78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
