{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KUxfXpimHyTD",
        "outputId": "66aba3f6-943a-4243-d99e-b3dbc90a5c18"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting ale\n",
            "  Downloading Ale-0.8.4.tar.gz (53 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/53.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.4/53.4 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: ale\n",
            "  Building wheel for ale (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ale: filename=Ale-0.8.4-py3-none-any.whl size=70154 sha256=a51d2573ec74dc782a672ef2a958de43cbd30499006aeaf715c9279e39769410\n",
            "  Stored in directory: /root/.cache/pip/wheels/2d/01/eb/61d0ee426a7f13c4d898c01b266ab2fbecf9ba0cb87e53df21\n",
            "Successfully built ale\n",
            "Installing collected packages: ale\n",
            "Successfully installed ale-0.8.4\n"
          ]
        }
      ],
      "source": [
        "!pip install ale"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gqgl3Xq6Hq_4",
        "outputId": "68583592-1459-4e64-813b-a8d67ddca727"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting shimmy\n",
            "  Downloading Shimmy-2.0.0-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.12/dist-packages (from shimmy) (2.0.2)\n",
            "Requirement already satisfied: gymnasium>=1.0.0a1 in /usr/local/lib/python3.12/dist-packages (from shimmy) (1.2.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium>=1.0.0a1->shimmy) (3.1.2)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium>=1.0.0a1->shimmy) (4.15.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from gymnasium>=1.0.0a1->shimmy) (0.0.4)\n",
            "Downloading Shimmy-2.0.0-py3-none-any.whl (30 kB)\n",
            "Installing collected packages: shimmy\n",
            "Successfully installed shimmy-2.0.0\n"
          ]
        }
      ],
      "source": [
        "!pip install shimmy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YyCp0qZGH7qx"
      },
      "source": [
        "# Preguntas del laboratorio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jp6pEnTaH-JE"
      },
      "source": [
        "\n",
        "**Objetivos y mecánicas del juego:**\n",
        "\n",
        "\n",
        "Basicamente Galaxian es un juego donde una nave debe dispararle a otras para ganar puntos. El objetivo del juego es matar a todas las naves posibles a lo largo de 3 vidas. Las naves enemigas disparan y pueden hacer ataques suicida por asi decirlo. El exito se mide en el puntaje.\n",
        "\n",
        "**Definición del estado del entorno:**\n",
        "\n",
        "El entorno ALE/Galaxian-v5 puede generar tres tipos de observación:\n",
        "\n",
        "1. RGB: Representa el entorno con imágenes a color de tamaño (210, 160, 3) píxeles.\n",
        "2. Grayscale: Versión en escala de grises de las imágenes RGB, con tamaño (210, 160).\n",
        "3. RAM: Estado comprimido de la memoria del Atari, representado como un vector de 128 valores enteros (0–255).\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**Acciones disponibles**\n",
        "\n",
        "El espacio tiene 6 acciones.\n",
        "\n",
        "1. NOOP: No realiza ninguna acción.\n",
        "2. FIRE: Dispara un proyectil hacia arriba.\n",
        "3. RIGHT: Mueve la nave hacia la derecha.\n",
        "4. LEFT: Mueve la nave hacia la izquierda.\n",
        "5. RIGHTFIRE: Mueve a la derecha y dispara simultáneamente.\n",
        "6. LEFTFIRE: Mueve a la izquierda y dispara simultáneamente.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dLrRp2gWNfxU",
        "outputId": "8dabe598-4471-45b6-bd95-77dea9226cf4"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from collections import deque\n",
        "import random\n",
        "from datetime import datetime\n",
        "import cv2\n",
        "import os\n",
        "import ale_py\n",
        "import shimmy\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "class DQN(nn.Module):\n",
        "    \"\"\"Red Neuronal Convolucional para Deep Q-Learning\"\"\"\n",
        "\n",
        "    def __init__(self, input_shape, n_actions):\n",
        "        super(DQN, self).__init__()\n",
        "\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        conv_out_size = self._get_conv_output(input_shape)\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(conv_out_size, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, n_actions)\n",
        "        )\n",
        "\n",
        "    def _get_conv_output(self, shape):\n",
        "        with torch.no_grad():\n",
        "            dummy_input = torch.zeros(1, *shape)\n",
        "            output = self.conv(dummy_input)\n",
        "            return int(np.prod(output.size()))\n",
        "\n",
        "    def forward(self, x):\n",
        "        conv_out = self.conv(x)\n",
        "        conv_out = conv_out.view(conv_out.size(0), -1)\n",
        "        return self.fc(conv_out)\n",
        "\n",
        "\n",
        "class ReplayBuffer:\n",
        "    \"\"\"Memoria de experiencias para entrenamiento DQN\"\"\"\n",
        "\n",
        "    def __init__(self, capacity=50000, device=\"cpu\"):\n",
        "        self.buffer = deque(maxlen=capacity)\n",
        "        self.device = device\n",
        "\n",
        "    def push(self, state, action, reward, next_state, done):\n",
        "        if len(self.buffer) == self.buffer.maxlen:\n",
        "            self.buffer.pop()\n",
        "        self.buffer.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def sample(self, batch_size=32):\n",
        "        transitions = random.sample(self.buffer, batch_size)\n",
        "        batch_state, batch_action, batch_reward, batch_next_state, batch_done = zip(*transitions)\n",
        "\n",
        "        batch_state = torch.FloatTensor(np.array(batch_state)).to(self.device)\n",
        "        batch_action = torch.LongTensor(batch_action).to(self.device)\n",
        "        batch_reward = torch.FloatTensor(batch_reward).to(self.device)\n",
        "        batch_next_state = torch.FloatTensor(np.array(batch_next_state)).to(self.device)\n",
        "        batch_done = torch.FloatTensor(batch_done).to(self.device)\n",
        "\n",
        "        return batch_state, batch_action, batch_reward, batch_next_state, batch_done\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "\n",
        "class DQNAgent:\n",
        "    \"\"\"Agente DQN para jugar Galaxian\"\"\"\n",
        "\n",
        "    def __init__(self, state_shape, n_actions, learning_rate=0.00025, gamma=0.99,\n",
        "                 epsilon_start=1.0, epsilon_end=0.01, epsilon_decay=0.995, replay_buffer_capacity=100000):\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.n_actions = n_actions\n",
        "        self.gamma = gamma\n",
        "        self.epsilon = epsilon_start\n",
        "        self.epsilon_end = epsilon_end\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "\n",
        "        self.policy_net = DQN(state_shape, n_actions).to(self.device)\n",
        "        self.target_net = DQN(state_shape, n_actions).to(self.device)\n",
        "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
        "        self.target_net.eval()\n",
        "\n",
        "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=learning_rate)\n",
        "        self.memory = ReplayBuffer(replay_buffer_capacity, device=self.device)\n",
        "\n",
        "        # Contador para entrenar menos frecuentemente\n",
        "        self.train_counter = 0\n",
        "\n",
        "    def select_action(self, state, training=True):\n",
        "        if training and random.random() < self.epsilon:\n",
        "            return random.randrange(self.n_actions)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
        "            q_values = self.policy_net(state_tensor)\n",
        "            return q_values.max(1)[1].item()\n",
        "\n",
        "    def train_step(self, batch_size=32):\n",
        "        if len(self.memory) < batch_size:\n",
        "            return None\n",
        "\n",
        "        # Solo entrenar cada 4 pasos para estabilidad\n",
        "        self.train_counter += 1\n",
        "        if self.train_counter % 4 != 0:\n",
        "            return None\n",
        "\n",
        "        batch_state, batch_action, batch_reward, batch_next_state, batch_done = self.memory.sample(batch_size)\n",
        "\n",
        "        # Clip de recompensas para estabilidad\n",
        "        batch_reward = torch.clamp(batch_reward, -1, 1)\n",
        "\n",
        "        current_q = self.policy_net(batch_state).gather(1, batch_action.unsqueeze(1))\n",
        "\n",
        "        with torch.no_grad():\n",
        "            next_q = self.target_net(batch_next_state).max(1)[0]\n",
        "            target_q = batch_reward + (1 - batch_done) * self.gamma * next_q\n",
        "\n",
        "        loss = nn.SmoothL1Loss()(current_q.squeeze(), target_q)\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        # Gradient clipping para evitar explosión de gradientes\n",
        "        torch.nn.utils.clip_grad_norm_(self.policy_net.parameters(), 10.0)\n",
        "        self.optimizer.step()\n",
        "\n",
        "        return loss.item()\n",
        "\n",
        "    def update_target_network(self):\n",
        "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
        "\n",
        "    def decay_epsilon(self):\n",
        "        self.epsilon = max(self.epsilon_end, self.epsilon * self.epsilon_decay)\n",
        "\n",
        "    def save(self, filepath):\n",
        "        torch.save({\n",
        "            'policy_net_state_dict': self.policy_net.state_dict(),\n",
        "            'target_net_state_dict': self.target_net.state_dict(),\n",
        "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
        "            'epsilon': self.epsilon\n",
        "        }, filepath)\n",
        "\n",
        "    def load(self, filepath):\n",
        "        checkpoint = torch.load(filepath, map_location=self.device, weights_only=False)\n",
        "        self.policy_net.load_state_dict(checkpoint['policy_net_state_dict'])\n",
        "        self.target_net.load_state_dict(checkpoint['target_net_state_dict'])\n",
        "        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "        self.epsilon = checkpoint['epsilon']\n",
        "\n",
        "\n",
        "def preprocess_frame(frame):\n",
        "    \"\"\"Preprocesa un frame del juego para la red neuronal\"\"\"\n",
        "    gray = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
        "    resized = cv2.resize(gray, (84, 84), interpolation=cv2.INTER_AREA)\n",
        "    normalized = resized.astype(np.float32) / 255.0\n",
        "    return normalized\n",
        "\n",
        "\n",
        "def stack_frames(frames, new_frame, is_new_episode):\n",
        "    \"\"\"Apila 4 frames consecutivos para dar noción de movimiento\"\"\"\n",
        "    if is_new_episode:\n",
        "        frames.clear()\n",
        "        for _ in range(4):\n",
        "            frames.append(new_frame)\n",
        "    else:\n",
        "        frames.append(new_frame)\n",
        "\n",
        "    return np.stack(frames, axis=0)\n",
        "\n",
        "\n",
        "def plot_training_metrics(rewards, losses, durations, email):\n",
        "    \"\"\"Genera gráficas de las métricas de entrenamiento\"\"\"\n",
        "    email_prefix = email.split(\"@\")[0]\n",
        "\n",
        "    fig, axes = plt.subplots(3, 1, figsize=(12, 10))\n",
        "    fig.suptitle('Métricas de Entrenamiento DQN - Galaxian', fontsize=16, fontweight='bold')\n",
        "\n",
        "    # Gráfica de recompensas\n",
        "    axes[0].plot(rewards, color='#2E86AB', linewidth=1.5, alpha=0.7)\n",
        "    axes[0].set_xlabel('Episodio', fontsize=11)\n",
        "    axes[0].set_ylabel('Recompensa Total', fontsize=11)\n",
        "    axes[0].set_title('Recompensa por Episodio', fontsize=12, fontweight='bold')\n",
        "    axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "    # Media móvil de recompensas (últimos 10 episodios)\n",
        "    if len(rewards) >= 10:\n",
        "        moving_avg = np.convolve(rewards, np.ones(10)/10, mode='valid')\n",
        "        axes[0].plot(range(9, len(rewards)), moving_avg, color='#A23B72',\n",
        "                     linewidth=2, label='Media Móvil (10 episodios)')\n",
        "        axes[0].legend()\n",
        "\n",
        "    # Gráfica de pérdidas\n",
        "    if losses:\n",
        "        axes[1].plot(losses, color='#F18F01', linewidth=1, alpha=0.7)\n",
        "        axes[1].set_xlabel('Episodio', fontsize=11)\n",
        "        axes[1].set_ylabel('Pérdida Promedio', fontsize=11)\n",
        "        axes[1].set_title('Pérdida de la Red Neuronal', fontsize=12, fontweight='bold')\n",
        "        axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "        # Media móvil de pérdidas\n",
        "        if len(losses) >= 10:\n",
        "            loss_moving_avg = np.convolve(losses, np.ones(10)/10, mode='valid')\n",
        "            axes[1].plot(range(9, len(losses)), loss_moving_avg, color='#C73E1D',\n",
        "                        linewidth=2, label='Media Móvil (10 episodios)')\n",
        "            axes[1].legend()\n",
        "\n",
        "    # Gráfica de duración de episodios\n",
        "    axes[2].plot(durations, color='#6A994E', linewidth=1.5, alpha=0.7)\n",
        "    axes[2].set_xlabel('Episodio', fontsize=11)\n",
        "    axes[2].set_ylabel('Duración (pasos)', fontsize=11)\n",
        "    axes[2].set_title('Duración del Episodio', fontsize=12, fontweight='bold')\n",
        "    axes[2].grid(True, alpha=0.3)\n",
        "\n",
        "    # Media móvil de duraciones\n",
        "    if len(durations) >= 10:\n",
        "        duration_moving_avg = np.convolve(durations, np.ones(10)/10, mode='valid')\n",
        "        axes[2].plot(range(9, len(durations)), duration_moving_avg, color='#386641',\n",
        "                     linewidth=2, label='Media Móvil (10 episodios)')\n",
        "        axes[2].legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Guardar gráfica\n",
        "    plot_filename = f'training_metrics_{email_prefix}.png'\n",
        "    plt.savefig(plot_filename, dpi=150, bbox_inches='tight')\n",
        "    print(f\"Gráfica guardada: {plot_filename}\")\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def record_episode(policy, email=\"estudiante@uvg.edu.gt\", output_dir=\"videos\"):\n",
        "    \"\"\"Graba un episodio completo usando la política proporcionada\"\"\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    env = gym.make('ALE/Galaxian-v5', render_mode='rgb_array')\n",
        "\n",
        "    state, info = env.reset()\n",
        "    done = False\n",
        "    truncated = False\n",
        "    total_reward = 0\n",
        "    frames = []\n",
        "\n",
        "    stacked_frames = deque(maxlen=4)\n",
        "    processed_frame = preprocess_frame(state)\n",
        "    state_stack = stack_frames(stacked_frames, processed_frame, True)\n",
        "\n",
        "    print(\"Iniciando grabación del episodio...\")\n",
        "\n",
        "    while not (done or truncated):\n",
        "        frame = env.render()\n",
        "        frames.append(frame)\n",
        "\n",
        "        action = policy.select_action(state_stack, training=False)\n",
        "\n",
        "        next_state, reward, done, truncated, info = env.step(action)\n",
        "        total_reward += reward\n",
        "\n",
        "        processed_frame = preprocess_frame(next_state)\n",
        "        state_stack = stack_frames(stacked_frames, processed_frame, False)\n",
        "\n",
        "    env.close()\n",
        "\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d%H%M\")\n",
        "    email_prefix = email.split('@')[0]\n",
        "    score = int(total_reward)\n",
        "    filename = f\"{email_prefix}_{timestamp}_{score}.mp4\"\n",
        "    filepath = os.path.join(output_dir, filename)\n",
        "\n",
        "    print(f\"Guardando video: {filepath}\")\n",
        "    print(f\"Puntuación obtenida: {score}\")\n",
        "    print(f\"Total de frames: {len(frames)}\")\n",
        "\n",
        "    if len(frames) > 0:\n",
        "        height, width = frames[0].shape[:2]\n",
        "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "        out = cv2.VideoWriter(filepath, fourcc, 30.0, (width, height))\n",
        "\n",
        "        for frame in frames:\n",
        "            frame_bgr = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
        "            out.write(frame_bgr)\n",
        "\n",
        "        out.release()\n",
        "        print(f\"Video guardado exitosamente: {filepath}\")\n",
        "    else:\n",
        "        print(\"No frames to save for the video.\")\n",
        "\n",
        "    return filepath\n",
        "\n",
        "\n",
        "def train_agent(episodes=15, email=\"estudiante@uvg.edu.gt\", checkpoint_path=None):\n",
        "    env = gym.make('ALE/Galaxian-v5')\n",
        "\n",
        "    state_shape = (4, 84, 84)\n",
        "    n_actions = env.action_space.n\n",
        "\n",
        "    agent = DQNAgent(state_shape, n_actions)\n",
        "\n",
        "    stacked_frames = deque(maxlen=4)\n",
        "    update_target_frequency = 10000  # Actualizar target network menos frecuentemente\n",
        "    steps = 0\n",
        "    episode_start = 0\n",
        "\n",
        "    # Listas para almacenar métricas\n",
        "    episode_rewards = []\n",
        "    episode_losses = []\n",
        "    episode_durations = []\n",
        "\n",
        "    if checkpoint_path and os.path.exists(checkpoint_path):\n",
        "        print(f\"Cargando checkpoint desde: {checkpoint_path}\")\n",
        "        checkpoint = torch.load(checkpoint_path, map_location=agent.device, weights_only=False)\n",
        "\n",
        "        agent.policy_net.load_state_dict(checkpoint['policy_net_state_dict'])\n",
        "        agent.target_net.load_state_dict(checkpoint['target_net_state_dict'])\n",
        "        agent.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "        agent.epsilon = checkpoint['epsilon']\n",
        "        episode_start = checkpoint.get('episode', 0)\n",
        "        steps = checkpoint.get('steps', 0)\n",
        "\n",
        "        # Cargar métricas previas si existen\n",
        "        episode_rewards = checkpoint.get('episode_rewards', [])\n",
        "        episode_losses = checkpoint.get('episode_losses', [])\n",
        "        episode_durations = checkpoint.get('episode_durations', [])\n",
        "\n",
        "        # Reiniciar el replay buffer (en lugar de cargarlo)\n",
        "        agent.memory = ReplayBuffer(capacity=100000, device=agent.device)\n",
        "\n",
        "        print(f\"Checkpoint cargado. Continuando desde episodio {episode_start}\")\n",
        "        print(f\"Epsilon actual: {agent.epsilon:.3f}\")\n",
        "\n",
        "    print(f\"\\nIniciando entrenamiento por {episodes} episodios...\")\n",
        "    print(f\"Dispositivo: {agent.device}\")\n",
        "    print(f\"Acciones disponibles: {n_actions}\")\n",
        "\n",
        "    for episode in range(episode_start, episode_start + episodes):\n",
        "        state, _ = env.reset()\n",
        "        processed_frame = preprocess_frame(state)\n",
        "        state_stack = stack_frames(stacked_frames, processed_frame, True)\n",
        "\n",
        "        episode_reward = 0\n",
        "        episode_loss_sum = 0\n",
        "        episode_loss_count = 0\n",
        "        episode_duration = 0\n",
        "        done = False\n",
        "        truncated = False\n",
        "\n",
        "        while not (done or truncated):\n",
        "            action = agent.select_action(state_stack, training=True)\n",
        "            next_state, reward, done, truncated, _ = env.step(action)\n",
        "\n",
        "            processed_frame = preprocess_frame(next_state)\n",
        "            next_state_stack = stack_frames(stacked_frames, processed_frame, False)\n",
        "\n",
        "            # Clip de recompensas para estabilidad\n",
        "            clipped_reward = np.clip(reward, -1, 1)\n",
        "\n",
        "            agent.memory.push(state_stack, action, clipped_reward, next_state_stack, done)\n",
        "\n",
        "            loss = agent.train_step()\n",
        "            if loss is not None:\n",
        "                episode_loss_sum += loss\n",
        "                episode_loss_count += 1\n",
        "\n",
        "            episode_reward += reward\n",
        "            episode_duration += 1\n",
        "            state_stack = next_state_stack\n",
        "            steps += 1\n",
        "\n",
        "            if steps % update_target_frequency == 0:\n",
        "                agent.update_target_network()\n",
        "                print(f\"  >> Target network actualizada en paso {steps}\")\n",
        "\n",
        "        agent.decay_epsilon()\n",
        "\n",
        "        # Guardar métricas del episodio\n",
        "        episode_rewards.append(episode_reward)\n",
        "        episode_durations.append(episode_duration)\n",
        "\n",
        "        avg_loss = episode_loss_sum / episode_loss_count if episode_loss_count > 0 else 0\n",
        "        episode_losses.append(avg_loss)\n",
        "\n",
        "        print(f\"Episodio {episode + 1}/{episode_start + episodes} | \"\n",
        "              f\"Recompensa: {episode_reward:.2f} | \"\n",
        "              f\"Duración: {episode_duration} pasos | \"\n",
        "              f\"Pérdida: {avg_loss:.4f} | \"\n",
        "              f\"Epsilon: {agent.epsilon:.3f} | \"\n",
        "              f\"Buffer: {len(agent.memory)}\")\n",
        "\n",
        "    # Guardar modelo final (último modelo)\n",
        "    email_prefix = email.split(\"@\")[0]\n",
        "    final_model_path = f'final_model_{email_prefix}.pth'\n",
        "\n",
        "    torch.save({\n",
        "        'policy_net_state_dict': agent.policy_net.state_dict(),\n",
        "        'target_net_state_dict': agent.target_net.state_dict(),\n",
        "        'optimizer_state_dict': agent.optimizer.state_dict(),\n",
        "        'epsilon': agent.epsilon,\n",
        "        'episode': episode_start + episodes,\n",
        "        'steps': steps,\n",
        "        'replay_buffer': agent.memory.buffer,  # Puedes elegir no guardar el buffer aquí si no lo quieres\n",
        "    }, final_model_path)\n",
        "\n",
        "    print(f\"\\nModelo final guardado: {final_model_path}\")\n",
        "\n",
        "    # Generar gráficas\n",
        "    print(\"\\nGenerando gráficas de métricas...\")\n",
        "    plot_training_metrics(episode_rewards, episode_losses, episode_durations, email)\n",
        "\n",
        "    # Grabar video al final del entrenamiento\n",
        "    print(f\"\\nGrabando video del modelo entrenado...\")\n",
        "    record_episode(agent, email)\n",
        "\n",
        "    env.close()\n",
        "    print(\"\\n=== Entrenamiento completado ===\")\n",
        "    print(f\"Episodios totales: {episode_start + episodes}\")\n",
        "    print(f\"Recompensa final: {episode_rewards[-1]:.2f}\")\n",
        "    print(f\"Recompensa promedio: {np.mean(episode_rewards):.2f}\")\n",
        "    print(f\"Epsilon final: {agent.epsilon:.3f}\")\n",
        "\n",
        "    return agent\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o2wANM-ZHxJ8"
      },
      "source": [
        "## ENTRENAMIENTO INICIAL (15 EPISODIOS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "YcsaOeUs8iXh",
        "outputId": "b36451e1-c96c-4859-e37b-b8dfe339b5c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "ENTRENAMIENTO INICIAL - DQN Galaxian\n",
            "======================================================================\n",
            "\n",
            "Iniciando entrenamiento por 100 episodios...\n",
            "Dispositivo: cpu\n",
            "Acciones disponibles: 6\n",
            "Episodio 1/100 | Recompensa: 660.00 | Duración: 511 pasos | Pérdida: 0.0163 | Epsilon: 0.995 | Buffer: 511\n",
            "Episodio 2/100 | Recompensa: 730.00 | Duración: 497 pasos | Pérdida: 0.0131 | Epsilon: 0.990 | Buffer: 1008\n",
            "Episodio 3/100 | Recompensa: 150.00 | Duración: 283 pasos | Pérdida: 0.0133 | Epsilon: 0.985 | Buffer: 1291\n",
            "Episodio 4/100 | Recompensa: 660.00 | Duración: 447 pasos | Pérdida: 0.0106 | Epsilon: 0.980 | Buffer: 1738\n",
            "Episodio 5/100 | Recompensa: 730.00 | Duración: 683 pasos | Pérdida: 0.0110 | Epsilon: 0.975 | Buffer: 2421\n",
            "Episodio 6/100 | Recompensa: 510.00 | Duración: 323 pasos | Pérdida: 0.0101 | Epsilon: 0.970 | Buffer: 2744\n",
            "Episodio 7/100 | Recompensa: 1030.00 | Duración: 625 pasos | Pérdida: 0.0133 | Epsilon: 0.966 | Buffer: 3369\n",
            "Episodio 8/100 | Recompensa: 980.00 | Duración: 841 pasos | Pérdida: 0.0118 | Epsilon: 0.961 | Buffer: 4210\n",
            "Episodio 9/100 | Recompensa: 830.00 | Duración: 437 pasos | Pérdida: 0.0118 | Epsilon: 0.956 | Buffer: 4647\n",
            "Episodio 10/100 | Recompensa: 150.00 | Duración: 287 pasos | Pérdida: 0.0118 | Epsilon: 0.951 | Buffer: 4934\n",
            "Episodio 11/100 | Recompensa: 1640.00 | Duración: 1309 pasos | Pérdida: 0.0109 | Epsilon: 0.946 | Buffer: 6243\n",
            "Episodio 12/100 | Recompensa: 270.00 | Duración: 253 pasos | Pérdida: 0.0119 | Epsilon: 0.942 | Buffer: 6496\n",
            "Episodio 13/100 | Recompensa: 920.00 | Duración: 931 pasos | Pérdida: 0.0103 | Epsilon: 0.937 | Buffer: 7427\n",
            "Episodio 14/100 | Recompensa: 950.00 | Duración: 491 pasos | Pérdida: 0.0096 | Epsilon: 0.932 | Buffer: 7918\n",
            "Episodio 15/100 | Recompensa: 780.00 | Duración: 573 pasos | Pérdida: 0.0120 | Epsilon: 0.928 | Buffer: 8491\n",
            "Episodio 16/100 | Recompensa: 1170.00 | Duración: 637 pasos | Pérdida: 0.0110 | Epsilon: 0.923 | Buffer: 9128\n",
            "Episodio 17/100 | Recompensa: 810.00 | Duración: 583 pasos | Pérdida: 0.0121 | Epsilon: 0.918 | Buffer: 9711\n",
            "  >> Target network actualizada en paso 10000\n",
            "Episodio 18/100 | Recompensa: 430.00 | Duración: 309 pasos | Pérdida: 0.0100 | Epsilon: 0.914 | Buffer: 10020\n",
            "Episodio 19/100 | Recompensa: 750.00 | Duración: 483 pasos | Pérdida: 0.0153 | Epsilon: 0.909 | Buffer: 10503\n",
            "Episodio 20/100 | Recompensa: 490.00 | Duración: 313 pasos | Pérdida: 0.0162 | Epsilon: 0.905 | Buffer: 10816\n",
            "Episodio 21/100 | Recompensa: 400.00 | Duración: 375 pasos | Pérdida: 0.0121 | Epsilon: 0.900 | Buffer: 11191\n",
            "Episodio 22/100 | Recompensa: 730.00 | Duración: 529 pasos | Pérdida: 0.0128 | Epsilon: 0.896 | Buffer: 11720\n",
            "Episodio 23/100 | Recompensa: 370.00 | Duración: 269 pasos | Pérdida: 0.0159 | Epsilon: 0.891 | Buffer: 11989\n",
            "Episodio 24/100 | Recompensa: 1510.00 | Duración: 1103 pasos | Pérdida: 0.0134 | Epsilon: 0.887 | Buffer: 13092\n",
            "Episodio 25/100 | Recompensa: 450.00 | Duración: 315 pasos | Pérdida: 0.0139 | Epsilon: 0.882 | Buffer: 13407\n",
            "Episodio 26/100 | Recompensa: 640.00 | Duración: 813 pasos | Pérdida: 0.0137 | Epsilon: 0.878 | Buffer: 14220\n",
            "Episodio 27/100 | Recompensa: 630.00 | Duración: 539 pasos | Pérdida: 0.0122 | Epsilon: 0.873 | Buffer: 14759\n",
            "Episodio 28/100 | Recompensa: 640.00 | Duración: 435 pasos | Pérdida: 0.0119 | Epsilon: 0.869 | Buffer: 15194\n",
            "Episodio 29/100 | Recompensa: 890.00 | Duración: 835 pasos | Pérdida: 0.0121 | Epsilon: 0.865 | Buffer: 16029\n",
            "Episodio 30/100 | Recompensa: 1470.00 | Duración: 1131 pasos | Pérdida: 0.0106 | Epsilon: 0.860 | Buffer: 17160\n",
            "Episodio 31/100 | Recompensa: 1000.00 | Duración: 875 pasos | Pérdida: 0.0107 | Epsilon: 0.856 | Buffer: 18035\n",
            "Episodio 32/100 | Recompensa: 770.00 | Duración: 567 pasos | Pérdida: 0.0104 | Epsilon: 0.852 | Buffer: 18602\n",
            "Episodio 33/100 | Recompensa: 730.00 | Duración: 489 pasos | Pérdida: 0.0102 | Epsilon: 0.848 | Buffer: 19091\n",
            "Episodio 34/100 | Recompensa: 760.00 | Duración: 871 pasos | Pérdida: 0.0091 | Epsilon: 0.843 | Buffer: 19962\n",
            "  >> Target network actualizada en paso 20000\n",
            "Episodio 35/100 | Recompensa: 870.00 | Duración: 539 pasos | Pérdida: 0.0142 | Epsilon: 0.839 | Buffer: 20501\n",
            "Episodio 36/100 | Recompensa: 770.00 | Duración: 617 pasos | Pérdida: 0.0150 | Epsilon: 0.835 | Buffer: 21118\n",
            "Episodio 37/100 | Recompensa: 420.00 | Duración: 359 pasos | Pérdida: 0.0125 | Epsilon: 0.831 | Buffer: 21477\n",
            "Episodio 38/100 | Recompensa: 270.00 | Duración: 343 pasos | Pérdida: 0.0121 | Epsilon: 0.827 | Buffer: 21820\n",
            "Episodio 39/100 | Recompensa: 360.00 | Duración: 283 pasos | Pérdida: 0.0139 | Epsilon: 0.822 | Buffer: 22103\n",
            "Episodio 40/100 | Recompensa: 630.00 | Duración: 461 pasos | Pérdida: 0.0140 | Epsilon: 0.818 | Buffer: 22564\n",
            "Episodio 41/100 | Recompensa: 840.00 | Duración: 725 pasos | Pérdida: 0.0114 | Epsilon: 0.814 | Buffer: 23289\n",
            "Episodio 42/100 | Recompensa: 490.00 | Duración: 519 pasos | Pérdida: 0.0117 | Epsilon: 0.810 | Buffer: 23808\n",
            "Episodio 43/100 | Recompensa: 970.00 | Duración: 621 pasos | Pérdida: 0.0105 | Epsilon: 0.806 | Buffer: 24429\n",
            "Episodio 44/100 | Recompensa: 720.00 | Duración: 497 pasos | Pérdida: 0.0100 | Epsilon: 0.802 | Buffer: 24926\n",
            "Episodio 45/100 | Recompensa: 1120.00 | Duración: 883 pasos | Pérdida: 0.0113 | Epsilon: 0.798 | Buffer: 25809\n",
            "Episodio 46/100 | Recompensa: 930.00 | Duración: 549 pasos | Pérdida: 0.0092 | Epsilon: 0.794 | Buffer: 26358\n",
            "Episodio 47/100 | Recompensa: 840.00 | Duración: 415 pasos | Pérdida: 0.0104 | Epsilon: 0.790 | Buffer: 26773\n",
            "Episodio 48/100 | Recompensa: 330.00 | Duración: 311 pasos | Pérdida: 0.0108 | Epsilon: 0.786 | Buffer: 27084\n",
            "Episodio 49/100 | Recompensa: 570.00 | Duración: 449 pasos | Pérdida: 0.0112 | Epsilon: 0.782 | Buffer: 27533\n",
            "Episodio 50/100 | Recompensa: 270.00 | Duración: 265 pasos | Pérdida: 0.0112 | Epsilon: 0.778 | Buffer: 27798\n",
            "Episodio 51/100 | Recompensa: 710.00 | Duración: 635 pasos | Pérdida: 0.0092 | Epsilon: 0.774 | Buffer: 28433\n",
            "Episodio 52/100 | Recompensa: 1050.00 | Duración: 835 pasos | Pérdida: 0.0103 | Epsilon: 0.771 | Buffer: 29268\n",
            "Episodio 53/100 | Recompensa: 340.00 | Duración: 355 pasos | Pérdida: 0.0110 | Epsilon: 0.767 | Buffer: 29623\n",
            "Episodio 54/100 | Recompensa: 300.00 | Duración: 325 pasos | Pérdida: 0.0119 | Epsilon: 0.763 | Buffer: 29948\n",
            "  >> Target network actualizada en paso 30000\n",
            "Episodio 55/100 | Recompensa: 390.00 | Duración: 325 pasos | Pérdida: 0.0167 | Epsilon: 0.759 | Buffer: 30273\n",
            "Episodio 56/100 | Recompensa: 530.00 | Duración: 527 pasos | Pérdida: 0.0170 | Epsilon: 0.755 | Buffer: 30800\n",
            "Episodio 57/100 | Recompensa: 830.00 | Duración: 669 pasos | Pérdida: 0.0151 | Epsilon: 0.751 | Buffer: 31469\n",
            "Episodio 58/100 | Recompensa: 310.00 | Duración: 281 pasos | Pérdida: 0.0156 | Epsilon: 0.748 | Buffer: 31750\n"
          ]
        },
        {
          "ename": "MemoryError",
          "evalue": "Unable to allocate 3.45 MiB for an array with shape (32, 4, 84, 84) and data type float32",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[2], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m70\u001b[39m)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Entrenar desde cero (sin checkpoint)\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m agent \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_agent\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43mepisodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEPISODES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43memail\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEMAIL\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43mcheckpoint_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# None = entrenamiento desde cero\u001b[39;49;00m\n\u001b[0;32m     15\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m¡Entrenamiento inicial completado!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModelo guardado como: final_model_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mEMAIL\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m@\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "Cell \u001b[1;32mIn[1], line 369\u001b[0m, in \u001b[0;36mtrain_agent\u001b[1;34m(episodes, email, checkpoint_path)\u001b[0m\n\u001b[0;32m    365\u001b[0m clipped_reward \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mclip(reward, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    367\u001b[0m agent\u001b[38;5;241m.\u001b[39mmemory\u001b[38;5;241m.\u001b[39mpush(state_stack, action, clipped_reward, next_state_stack, done)\n\u001b[1;32m--> 369\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    370\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m loss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    371\u001b[0m     episode_loss_sum \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n",
            "Cell \u001b[1;32mIn[1], line 120\u001b[0m, in \u001b[0;36mDQNAgent.train_step\u001b[1;34m(self, batch_size)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_counter \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m4\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 120\u001b[0m batch_state, batch_action, batch_reward, batch_next_state, batch_done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmemory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;66;03m# Clip de recompensas para estabilidad\u001b[39;00m\n\u001b[0;32m    123\u001b[0m batch_reward \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mclamp(batch_reward, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n",
            "Cell \u001b[1;32mIn[1], line 70\u001b[0m, in \u001b[0;36mReplayBuffer.sample\u001b[1;34m(self, batch_size)\u001b[0m\n\u001b[0;32m     68\u001b[0m batch_action \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mLongTensor(batch_action)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m     69\u001b[0m batch_reward \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor(batch_reward)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m---> 70\u001b[0m batch_next_state \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_next_state\u001b[49m\u001b[43m)\u001b[49m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m     71\u001b[0m batch_done \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor(batch_done)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m batch_state, batch_action, batch_reward, batch_next_state, batch_done\n",
            "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 3.45 MiB for an array with shape (32, 4, 84, 84) and data type float32"
          ]
        }
      ],
      "source": [
        "EMAIL = \"estudiante@uvg.edu.gt\"\n",
        "\n",
        "# Número de episodios para entrenar\n",
        "EPISODES = 100\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"ENTRENAMIENTO INICIAL - DQN Galaxian\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Entrenar desde cero (sin checkpoint)\n",
        "agent = train_agent(\n",
        "episodes=EPISODES,\n",
        "email=EMAIL,\n",
        "checkpoint_path=None  # None = entrenamiento desde cero\n",
        ")\n",
        "\n",
        "print(\"\\n¡Entrenamiento inicial completado!\")\n",
        "print(f\"Modelo guardado como: final_model_{EMAIL.split('@')[0]}.pth\")\n",
        "print(f\"Gráficas guardadas como: training_metrics_{EMAIL.split('@')[0]}.png\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hv2-h2J4Hrj1"
      },
      "source": [
        "## REENTRENAR DESDE MODELO GUARDADO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vr0tW1-dN2ah",
        "outputId": "5474c5cb-3255-4687-c9ca-ac0d98ca9a68"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "CONTINUANDO ENTRENAMIENTO\n",
            "============================================================\n",
            "Modelo encontrado: final_model_ang23010.pth\n",
            "  - Episodio previo: 75\n",
            "  - Mejor score: N/A\n",
            "  - Epsilon: 0.6866\n",
            "\n",
            "Continuando por 15 episodios adicionales...\n",
            "Cargando checkpoint desde: final_model_ang23010.pth\n",
            "Checkpoint cargado. Continuando desde episodio 75\n",
            "Epsilon actual: 0.687\n",
            "\n",
            "Iniciando entrenamiento por 50 episodios...\n",
            "Dispositivo: cuda\n",
            "Acciones disponibles: 6\n",
            "Episodio 76/125 | Recompensa: 490.00 | Duración: 469 pasos | Pérdida: 0.0073 | Epsilon: 0.683 | Buffer: 469\n",
            "Episodio 77/125 | Recompensa: 1260.00 | Duración: 1137 pasos | Pérdida: 0.0084 | Epsilon: 0.680 | Buffer: 1606\n",
            "Episodio 78/125 | Recompensa: 670.00 | Duración: 347 pasos | Pérdida: 0.0055 | Epsilon: 0.676 | Buffer: 1953\n",
            "Episodio 79/125 | Recompensa: 930.00 | Duración: 741 pasos | Pérdida: 0.0083 | Epsilon: 0.673 | Buffer: 2694\n",
            "Episodio 80/125 | Recompensa: 330.00 | Duración: 321 pasos | Pérdida: 0.0075 | Epsilon: 0.670 | Buffer: 3015\n",
            "Episodio 81/125 | Recompensa: 980.00 | Duración: 953 pasos | Pérdida: 0.0078 | Epsilon: 0.666 | Buffer: 3968\n",
            "Episodio 82/125 | Recompensa: 900.00 | Duración: 615 pasos | Pérdida: 0.0078 | Epsilon: 0.663 | Buffer: 4583\n",
            "Episodio 83/125 | Recompensa: 920.00 | Duración: 773 pasos | Pérdida: 0.0075 | Epsilon: 0.660 | Buffer: 5356\n",
            "Episodio 84/125 | Recompensa: 480.00 | Duración: 391 pasos | Pérdida: 0.0082 | Epsilon: 0.656 | Buffer: 5747\n",
            "Episodio 85/125 | Recompensa: 1570.00 | Duración: 1269 pasos | Pérdida: 0.0081 | Epsilon: 0.653 | Buffer: 7016\n",
            "Episodio 86/125 | Recompensa: 730.00 | Duración: 529 pasos | Pérdida: 0.0093 | Epsilon: 0.650 | Buffer: 7545\n",
            "Episodio 87/125 | Recompensa: 650.00 | Duración: 543 pasos | Pérdida: 0.0087 | Epsilon: 0.647 | Buffer: 8088\n",
            "Episodio 88/125 | Recompensa: 660.00 | Duración: 457 pasos | Pérdida: 0.0087 | Epsilon: 0.643 | Buffer: 8545\n",
            "Episodio 89/125 | Recompensa: 340.00 | Duración: 265 pasos | Pérdida: 0.0090 | Epsilon: 0.640 | Buffer: 8810\n",
            "Episodio 90/125 | Recompensa: 700.00 | Duración: 645 pasos | Pérdida: 0.0078 | Epsilon: 0.637 | Buffer: 9455\n",
            "Episodio 91/125 | Recompensa: 500.00 | Duración: 315 pasos | Pérdida: 0.0082 | Epsilon: 0.634 | Buffer: 9770\n",
            "  >> Target network actualizada en paso 10000\n",
            "Episodio 92/125 | Recompensa: 1670.00 | Duración: 1289 pasos | Pérdida: 0.0134 | Epsilon: 0.631 | Buffer: 11059\n",
            "Episodio 93/125 | Recompensa: 360.00 | Duración: 265 pasos | Pérdida: 0.0136 | Epsilon: 0.627 | Buffer: 11324\n",
            "Episodio 94/125 | Recompensa: 150.00 | Duración: 279 pasos | Pérdida: 0.0133 | Epsilon: 0.624 | Buffer: 11603\n",
            "Episodio 95/125 | Recompensa: 1100.00 | Duración: 727 pasos | Pérdida: 0.0110 | Epsilon: 0.621 | Buffer: 12330\n",
            "Episodio 96/125 | Recompensa: 670.00 | Duración: 541 pasos | Pérdida: 0.0105 | Epsilon: 0.618 | Buffer: 12871\n",
            "Episodio 97/125 | Recompensa: 810.00 | Duración: 577 pasos | Pérdida: 0.0100 | Epsilon: 0.615 | Buffer: 13448\n",
            "Episodio 98/125 | Recompensa: 540.00 | Duración: 313 pasos | Pérdida: 0.0102 | Epsilon: 0.612 | Buffer: 13761\n",
            "Episodio 99/125 | Recompensa: 1080.00 | Duración: 697 pasos | Pérdida: 0.0098 | Epsilon: 0.609 | Buffer: 14458\n",
            "Episodio 100/125 | Recompensa: 1000.00 | Duración: 879 pasos | Pérdida: 0.0102 | Epsilon: 0.606 | Buffer: 15337\n",
            "Episodio 101/125 | Recompensa: 1010.00 | Duración: 671 pasos | Pérdida: 0.0097 | Epsilon: 0.603 | Buffer: 16008\n",
            "Episodio 102/125 | Recompensa: 710.00 | Duración: 491 pasos | Pérdida: 0.0091 | Epsilon: 0.600 | Buffer: 16499\n",
            "Episodio 103/125 | Recompensa: 750.00 | Duración: 609 pasos | Pérdida: 0.0108 | Epsilon: 0.597 | Buffer: 17108\n",
            "Episodio 104/125 | Recompensa: 580.00 | Duración: 443 pasos | Pérdida: 0.0104 | Epsilon: 0.594 | Buffer: 17551\n",
            "Episodio 105/125 | Recompensa: 660.00 | Duración: 537 pasos | Pérdida: 0.0091 | Epsilon: 0.591 | Buffer: 18088\n",
            "Episodio 106/125 | Recompensa: 740.00 | Duración: 693 pasos | Pérdida: 0.0096 | Epsilon: 0.588 | Buffer: 18781\n",
            "Episodio 107/125 | Recompensa: 600.00 | Duración: 343 pasos | Pérdida: 0.0106 | Epsilon: 0.585 | Buffer: 19124\n",
            "Episodio 108/125 | Recompensa: 710.00 | Duración: 495 pasos | Pérdida: 0.0111 | Epsilon: 0.582 | Buffer: 19619\n",
            "Episodio 109/125 | Recompensa: 450.00 | Duración: 331 pasos | Pérdida: 0.0101 | Epsilon: 0.579 | Buffer: 19950\n",
            "  >> Target network actualizada en paso 20000\n",
            "Episodio 110/125 | Recompensa: 750.00 | Duración: 681 pasos | Pérdida: 0.0128 | Epsilon: 0.576 | Buffer: 20631\n",
            "Episodio 111/125 | Recompensa: 1230.00 | Duración: 799 pasos | Pérdida: 0.0128 | Epsilon: 0.573 | Buffer: 21430\n",
            "Episodio 112/125 | Recompensa: 570.00 | Duración: 367 pasos | Pérdida: 0.0134 | Epsilon: 0.570 | Buffer: 21797\n",
            "Episodio 113/125 | Recompensa: 1010.00 | Duración: 841 pasos | Pérdida: 0.0103 | Epsilon: 0.568 | Buffer: 22638\n",
            "Episodio 114/125 | Recompensa: 270.00 | Duración: 271 pasos | Pérdida: 0.0117 | Epsilon: 0.565 | Buffer: 22909\n",
            "Episodio 115/125 | Recompensa: 330.00 | Duración: 273 pasos | Pérdida: 0.0124 | Epsilon: 0.562 | Buffer: 23182\n",
            "Episodio 116/125 | Recompensa: 1070.00 | Duración: 661 pasos | Pérdida: 0.0103 | Epsilon: 0.559 | Buffer: 23843\n",
            "Episodio 117/125 | Recompensa: 1410.00 | Duración: 999 pasos | Pérdida: 0.0111 | Epsilon: 0.556 | Buffer: 24842\n",
            "Episodio 118/125 | Recompensa: 730.00 | Duración: 529 pasos | Pérdida: 0.0099 | Epsilon: 0.554 | Buffer: 25371\n",
            "Episodio 119/125 | Recompensa: 250.00 | Duración: 255 pasos | Pérdida: 0.0103 | Epsilon: 0.551 | Buffer: 25626\n",
            "Episodio 120/125 | Recompensa: 520.00 | Duración: 467 pasos | Pérdida: 0.0099 | Epsilon: 0.548 | Buffer: 26093\n",
            "Episodio 121/125 | Recompensa: 1510.00 | Duración: 1039 pasos | Pérdida: 0.0106 | Epsilon: 0.545 | Buffer: 27132\n",
            "Episodio 122/125 | Recompensa: 570.00 | Duración: 355 pasos | Pérdida: 0.0102 | Epsilon: 0.543 | Buffer: 27487\n",
            "Episodio 123/125 | Recompensa: 670.00 | Duración: 567 pasos | Pérdida: 0.0096 | Epsilon: 0.540 | Buffer: 28054\n",
            "Episodio 124/125 | Recompensa: 610.00 | Duración: 447 pasos | Pérdida: 0.0116 | Epsilon: 0.537 | Buffer: 28501\n",
            "Episodio 125/125 | Recompensa: 790.00 | Duración: 513 pasos | Pérdida: 0.0095 | Epsilon: 0.534 | Buffer: 29014\n",
            "\n",
            "Modelo final guardado: final_model_ang23010.pth\n",
            "\n",
            "Generando gráficas de métricas...\n",
            "Gráfica guardada: training_metrics_ang23010.png\n",
            "\n",
            "Grabando video del modelo entrenado...\n",
            "Iniciando grabación del episodio...\n",
            "Guardando video: videos/ang23010_202511111755_270.mp4\n",
            "Puntuación obtenida: 270\n",
            "Total de frames: 271\n",
            "Video guardado exitosamente: videos/ang23010_202511111755_270.mp4\n",
            "\n",
            "=== Entrenamiento completado ===\n",
            "Episodios totales: 125\n",
            "Recompensa final: 790.00\n",
            "Recompensa promedio: 759.80\n",
            "Epsilon final: 0.534\n"
          ]
        }
      ],
      "source": [
        "\n",
        "EMAIL = \"ang23010@uvg.edu.gt\"\n",
        "email_prefix = EMAIL.split(\"@\")[0]\n",
        "\n",
        "\n",
        "best_model_path = f'final_model_{email_prefix}.pth'\n",
        "checkpoint_final_path = f'checkpoint_final_{email_prefix}.pth'\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"CONTINUANDO ENTRENAMIENTO\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "if os.path.exists(best_model_path):\n",
        "    print(f\"Modelo encontrado: {best_model_path}\")\n",
        "\n",
        "    checkpoint = torch.load(best_model_path, map_location='cpu', weights_only=False)\n",
        "    print(f\"  - Episodio previo: {checkpoint.get('episode', 'N/A')}\")\n",
        "    best_score = checkpoint.get('best_score', 'N/A')\n",
        "    if isinstance(best_score, (int, float)):\n",
        "        print(f\"  - Mejor score: {best_score:.2f}\")\n",
        "    else:\n",
        "        print(f\"  - Mejor score: {best_score}\")\n",
        "    print(f\"  - Epsilon: {checkpoint.get('epsilon', 'N/A'):.4f}\")\n",
        "    print()\n",
        "\n",
        "    print(\"Continuando por 15 episodios adicionales...\")\n",
        "    agent = train_agent(\n",
        "        episodes=50,\n",
        "        email=EMAIL,\n",
        "        checkpoint_path=best_model_path\n",
        "    )\n",
        "else:\n",
        "    print(f\"No se encontró el modelo: {best_model_path}\")\n",
        "    print(\"Ejecutar primero CELDA 2 para entrenar el modelo inicial\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_LWcLRaHdQC"
      },
      "source": [
        "## Grabar episodio del modelo guardado"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dc908c01",
        "outputId": "a62d9148-b94b-4efd-a979-e7328d095e9e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "REPLICANDO MEJOR EPISODIO CON MODELO ENTRENADO\n",
            "============================================================\n",
            "Cargado el mejor modelo desde: best_model_ang23010.pth\n",
            "Iniciando grabación del episodio...\n",
            "Guardando video: videos/ang23010_202510300330_990.mp4\n",
            "Puntuación obtenida: 990\n",
            "Total de frames: 1027\n",
            "Video guardado exitosamente: videos/ang23010_202510300330_990.mp4\n",
            "✓ Video del mejor episodio grabado exitosamente\n"
          ]
        }
      ],
      "source": [
        "\n",
        "EMAIL = \"ang23010@uvg.edu.gt\"\n",
        "email_prefix = EMAIL.split(\"@\")[0]\n",
        "best_model_path = f'best_model_{email_prefix}.pth'\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"REPLICANDO MEJOR EPISODIO CON MODELO ENTRENADO\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "if os.path.exists(best_model_path):\n",
        "    state_shape = (4, 84, 84)\n",
        "    n_actions = 6\n",
        "\n",
        "    agent = DQNAgent(state_shape, n_actions)\n",
        "    agent.load(best_model_path)\n",
        "\n",
        "    print(f\"Cargado el mejor modelo desde: {best_model_path}\")\n",
        "\n",
        "    record_episode(agent, email=EMAIL)\n",
        "    print(\"Video del mejor episodio grabado exitosamente\")\n",
        "else:\n",
        "    print(f\"No se encontró el modelo: {best_model_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xvnpu_j9LcxR",
        "outputId": "c6d11591-6a7a-4764-a9e7-9c9a621bc179"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "76f71e24",
        "outputId": "f3a1380a-d5ff-4fd9-94ee-258f68b3e60c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[NbConvertApp] Converting notebook /content/drive/MyDrive/LAB_10_RL.ipynb to html\n",
            "[NbConvertApp] Writing 365373 bytes to /content/drive/MyDrive/LAB_10_RL.html\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import fnmatch\n",
        "\n",
        "def find_notebook_path(drive_path, notebook_name):\n",
        "    for root, _, files in os.walk(drive_path):\n",
        "        for filename in fnmatch.filter(files, notebook_name):\n",
        "            return os.path.join(root, filename)\n",
        "    return None\n",
        "\n",
        "notebook_name = \"LAB_10_RL.ipynb\"\n",
        "drive_path = '/content/drive/MyDrive'\n",
        "notebook_path = find_notebook_path(drive_path, notebook_name)\n",
        "\n",
        "!jupyter nbconvert --to html \"{notebook_path}\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "09wVTlF8LpyO"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "95a3ceda",
        "outputId": "b775767a-5bbd-4a9a-988a-56cecf78a177"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "'Colab Notebooks'\t\t\t  LAB_10_RL.ipynb   Untitled0.ipynb\n",
            "'Copia de Fireball Stats'$'\\n''.gsheet'   new_model.ipynb\n"
          ]
        }
      ],
      "source": [
        "!ls /content/drive/MyDrive/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VpW16Zm4L0Qh"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
